syntax = "proto3";

package xllm_service.llm.proto;

message Usage {
  // the number of tokens in the prompt.
  optional int32 prompt_tokens = 1 [json_name="prompt_tokens"];

  // the number of tokens in the generated completion.
  optional int32 completion_tokens = 2 [json_name="completion_tokens"];

  // the total number of tokens used in the request (prompt + completion).
  optional int32 total_tokens = 3 [json_name="total_tokens"];
}

// Options for streaming response.
message StreamOptions{
  // if set, an additional chunk with usage will be streamed before the data: [DONE] message.
  optional bool include_usage = 1;
}

message Status {
  bool ok = 1;
}

message Routing {
  string prefill_name = 1;

  string decode_name = 2;
}
